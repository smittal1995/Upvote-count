# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xmI7LU6wS3Kaflh8DpJm3hCn8W5AN32D
"""

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, Binarizer
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,r2_score

train = pd.read_csv('train_NIR5Yl1.csv')
train.head()

train.drop(['ID','Username'],axis=1,inplace=True)

bn = Binarizer(threshold=4)
pd_watched = bn.transform([train['Answers']])[0]
train['pd_watched'] = pd_watched

le = LabelEncoder()
train['Tag'] = le.fit_transform(train['Tag'])
print(train.head())

X=train.drop('Upvotes',axis=1)
y=train['Upvotes']

std=StandardScaler()
X_scaled=pd.DataFrame(std.fit_transform(X),columns=X.columns,index=X.index)

poly_reg=PolynomialFeatures(degree=4,include_bias=True,interaction_only=False)
X_poly_train = poly_reg.fit_transform(X_scaled)
X_poly_train = pd.DataFrame(X_poly_train)
X_poly_train

normalizer = preprocessing.Normalization()
normalizer.adapt(np.array(X_poly_train))

def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(128, activation=tf.nn.leaky_relu),
      layers.Dropout(0.3),      
      layers.Dense(256, activation=tf.nn.leaky_relu),
      layers.Dropout(0.4),
      layers.Dense(128, activation=tf.nn.leaky_relu),
      layers.Dropout(0.3),
     # layers.Dense(19, activation=tf.nn.leaky_relu),
     # layers.Dropout(0.2), 
      layers.Dense(1),
  ])
  rmse = tf.keras.metrics.RootMeanSquaredError()
  model.compile(loss=tf.keras.losses.MSE,
                optimizer=tf.keras.optimizers.Adam(0.0013),
                metrics=[
                  #  tf.keras.metrics.MAE,
                  #  tf.keras.metrics.MSE,
                   #r2_score, # -1 and 1, < 0 => useless, 0 and 1 => better close to 1
                   rmse,
                ]
                )
  return model

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, mode='auto',restore_best_weights=True)

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = dnn_model.fit(
#     X_poly_train, y,
#     validation_split=0.2,
#     batch_size=100000,
#     verbose=2, epochs=300, callbacks=[callback])

def plot_loss(history):
  plt.plot(history.history['root_mean_squared_error'], label='loss')
  plt.plot(history.history['val_root_mean_squared_error'], label='val_loss')
 # plt.ylim([10, 80])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)

plot_loss(history)

dnn_model.save('dnn_model')
reloaded = tf.keras.models.load_model('dnn_model')

test = pd.read_csv('test_8i3B3FC.csv')
ID = test['ID']
test.drop(['ID','Username'],axis=1,inplace=True)
test['Tag'] = le.fit_transform(test['Tag'])
pd_watched = bn.transform([test['Answers']])[0]
test['pd_watched'] = pd_watched

test_scaled=pd.DataFrame(std.fit_transform(test),columns=test.columns,index=test.index)
test_poly = poly_reg.fit_transform(test_scaled)
test_poly = pd.DataFrame(test_poly)
test_pred = reloaded.predict(test_poly)

ans = pd.DataFrame({'ID' : ID})
ans['Upvotes'] = test_pred
sub = ans.sort_values(by=['ID'])
print(sub)

sub.to_csv('dnn_sub-1.csv', index=False)