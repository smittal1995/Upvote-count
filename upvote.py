# -*- coding: utf-8 -*-
"""Upvote.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B2GuTUI48X-Wc687jeINRWi-xpZjaowE
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns


# Make numpy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

print(tf.__version__)

data = pd.read_csv('train_NIR5Yl1.csv').sample(frac=1)
print(data.describe())

features = data[['Reputation', 'Answers', 'Views', 'Tag']]
onehot_Tag = pd.get_dummies(features['Tag'])
features = features.drop(columns=['Tag'])
features = pd.concat([features, onehot_Tag], axis=1)
targets = data['Upvotes']

data_preprocessed = pd.concat([features, targets], axis=1)

train_dataset = data_preprocessed

#train_dataset = data_preprocessed.sample(frac=0.8, random_state=0)
#test_dataset = data_preprocessed.drop(train_dataset.index)

train_features = train_dataset.copy()
#test_features = test_dataset.copy()

train_labels = train_features.pop('Upvotes')
#test_labels = test_features.pop('Upvotes')
print(train_features)
print(train_labels)
#train_dataset.describe().transpose()[['mean', 'std']]

normalizer = preprocessing.Normalization()
normalizer.adapt(np.array(train_features))
#print(normalizer.mean.numpy())

def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true))) 

def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(128, activation='relu'),
      layers.Dropout(0.1),
      layers.Dense(128, activation='relu'),
      layers.Dropout(0.1),
      layers.Dense(128, activation='relu'),
      layers.Dropout(0.1),   
      layers.Dense(128, activation='relu'),
      layers.Dropout(0.1),   
      layers.Dense(1),
  ])
  rmse = tf.keras.metrics.RootMeanSquaredError()
  model.compile(loss=tf.keras.losses.MSE,
                optimizer=tf.keras.optimizers.Adam(0.0004),
                metrics=[
                  #  tf.keras.metrics.MAE,
                  #  tf.keras.metrics.MSE,
                    #R_squared, # -1 and 1, < 0 => useless, 0 and 1 => better close to 1
                   rmse,
                ]
                )
  return model

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20, mode='auto',restore_best_weights=True)

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = dnn_model.fit(
#     train_features, train_labels,
#     validation_split=0.2,
#     verbose=2, epochs=100, callbacks=[callback])

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
 # plt.ylim([10, 80])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)

plot_loss(history)

data1 = pd.read_csv('test_8i3B3FC.csv').sample(frac=1)
data1.describe()

features = data1[['Reputation', 'Answers', 'Views', 'Tag']]
onehot_Tag = pd.get_dummies(features['Tag'])
features = features.drop(columns=['Tag'])
features = pd.concat([features, onehot_Tag], axis=1)


data_preprocessed = features

test_dataset = data_preprocessed

#train_dataset = data_preprocessed.sample(frac=0.8, random_state=0)
#test_dataset = data_preprocessed.drop(train_dataset.index)

test_features = test_dataset.copy()
#test_features = test_dataset.copy()


#test_labels = test_features.pop('Upvotes')

dnn_model.save('dnn_model')

reloaded = tf.keras.models.load_model('dnn_model')
temp = reloaded.predict(test_features)

test_features = data1[['ID']]
temp[temp<0]=0
test_features['Upvotes'] = temp

print(test_features)
sort = test_features.sort_values(by=['ID'])
print(sort)

sort.to_csv('Sub-8.csv', index=False)

